import random
import torch
from d2l import torch as d2l                    #d2l需要通过pip获取

# 根据w，b生成样本和标签
def synthetic_data(w,b,num_examples):
    x = torch.normal(0,1,(num_examples,len(w)))# 生成均值为0方差为1的随机数，大小为num_examples,列数为w长度
    y = torch.matmul(x,w)+b                    # 根据设定好的w，b生成y
    y+=torch.normal(0,0.01,y.shape)            # 加入一个均值为零，方差为一的噪声
    return x,y.reshape(-1,1)                   # -1表示，该维度的大小由pytorch自动得出
    
    
true_w = torch.tensor([2,-3.4])                # 设置线性方程w
true_b = 4.2                                   # 设置线性方程b


features,labels = synthetic_data(true_w,true_b,1000)  # 得到x与y值。x->features,y->labels

w = torch.normal(0,0.01,size=(2,1),requires_grad=True) # 设置预测w的初始值
b = torch.zeros(1,requires_grad=True)                  # 设置预测b的初始值
batch_size = 10                               # 设置每次获取的样本量

# 定义模型
def linreg(x,w,b):
    """线性回归模型"""
    return torch.matmul(x,w)+b
    
    
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))                                            
    random.shuffle(indices)                 # 这些样本是随机读取的，没有特定的顺序
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i:min(i +
                                                   batch_size, num_examples)]) # 当样本小于设定的获取量时，只读取剩余量
        yield features[batch_indices], labels[batch_indices]                   # 返回已读取的batch_size个样本和标签

def squared_loss(y_hat,y):
    """均方损失"""
    return (y_hat-y.reshape(y_hat.shape))**2/2   # y_hat指的是预测值，y指的是标签值
   
   
def sgd(params,lr,batch_size):
    """小批量随机梯度下降"""
    with torch.no_grad():                 #下面的程序不进行梯度计算
        for param in params:
            param -= lr*param.grad/batch_size   # 根据梯度以及设定的学习效率‘lr’调整param，也就是[w,b]的优值
            param.grad.zero_()
            
lr = 2
num_epochs = 3  # 要迭代的次数
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for x,y in data_iter(batch_size,features,labels):
        l = loss(net(x,w,b),y)                # x和y的小批量(batch_size)损失
                                              # 因为l的形状是(batch_size,1)，而不是一个标量。‘l’中所有的元素
                                              # 并以此计算关于[w,b]的梯度
        l.sum().backward()
        sgd([w,b],lr,batch_size)              # 根据梯度更新w和b的值,由于loss不是均差，因此sgd需要batch_size调整学习效率
    with torch.no_grad():
        train_l = loss(net(features,w,b),labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
print(f'w的估计误差：{true_w-w.reshape(true_w.shape)}')
print(f'b的估计误差：{true_b-b}')            
